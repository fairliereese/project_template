{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2404c29-2d11-4c4e-bd33-516103a286a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from marko import Markdown\n",
    "from marko.md_renderer import MarkdownRenderer\n",
    "from marko.inline import RawText, Link, CodeSpan\n",
    "from marko.block import FencedCode, CodeBlock\n",
    "\n",
    "import re\n",
    "\n",
    "# Append resources dir to path\n",
    "p = os.getcwd()+'/resources/'\n",
    "sys.path.append(p)\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4dda0201-6e71-4f07-b92a-7c5326ff2727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save mn5 version of config\n",
    "# save_mn5_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3ec675c-770f-4d54-98c8-6303f6469d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add links to subfolders for the analysis and processing folders\n",
    "def get_branch_url():\n",
    "    \"\"\"\n",
    "    Construct the GitHub URL for the current branch of the repository.\n",
    "\n",
    "    This function determines the name of the current Git branch using\n",
    "    ``git branch --show-current`` and constructs a URL pointing to the\n",
    "    root of that branch on GitHub. The base GitHub repository URL must\n",
    "    be provided by ``load_resources()`` as ``m['']``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A URL string pointing to the root of the current branch on GitHub,\n",
    "        formatted as: ``<gh_url>tree/<branch_name>/``.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Example output for the ``main`` branch:\n",
    "      ``https://github.com/user/repo/tree/main/``.\n",
    "    \"\"\"\n",
    "    m = load_resources()\n",
    "    cmd = 'git branch --show-current'\n",
    "    b = run_cmd(cmd)\n",
    "    b = b.strip()\n",
    "    branch_url = f\"{m['gh_url']}tree/{b}/\"\n",
    "    return branch_url\n",
    "\n",
    "def find_missing_subdirs(d):\n",
    "    \"\"\"\n",
    "    Identify subdirectories within a given directory that are not referenced in its README.\n",
    "\n",
    "    This function checks each subdirectory of the specified directory `d` and compares\n",
    "    its name against entries in the corresponding `README.md` file. Certain permanent\n",
    "    directories (e.g., 'rules', 'template_snakemake') are ignored. Subdirectories not\n",
    "    found in the README are collected and returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : str\n",
    "        Name of the parent directory to scan. Typically 'processing'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of pathlib.Path\n",
    "        List of subdirectory paths that are missing from the README. If all subdirectories\n",
    "        are listed in the README, the list will be empty.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Permanent directories defined for 'processing' are ['rules', 'template_snakemake'].\n",
    "    - The function assumes the README is located at '../<d>/README.md'.\n",
    "    - Only immediate subdirectories of `d` are checked, not nested ones.\n",
    "    \"\"\"\n",
    "\n",
    "    to_update = []\n",
    "\n",
    "    if d == 'processing':\n",
    "        perm_dirs = ['rules', 'template_snakemake', '.ipynb_checkpoints']\n",
    "    elif d == 'analysis':\n",
    "        perm_dirs = ['.ipynb_checkpoints']\n",
    "    else: perm_dirs = []\n",
    "\n",
    "    readme = f'{d}/README.md'\n",
    "    \n",
    "\n",
    "    # loop through each valid subdir\n",
    "    for sub_d in Path(f'{d}/').glob('*/'):\n",
    "        stem_sub_d = sub_d.stem\n",
    "        if stem_sub_d in perm_dirs: continue\n",
    "        if not sub_d.is_dir(): continue\n",
    "\n",
    "        fmt_sub_d = f'[{stem_sub_d}]'\n",
    "        if any(fmt_sub_d in line for line in open(readme)): continue\n",
    "        to_update.append(sub_d)\n",
    "        \n",
    "    return to_update\n",
    "\n",
    "def add_missing_subdirs_to_readme(d, missing_dirs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Append missing subdirectory entries to the appropriate section of a README.\n",
    "\n",
    "    This function reads the README file for a given top-level directory (`d`) and\n",
    "    appends bullet points for any subdirectories listed in `missing_dirs` that\n",
    "    are not already present. The new bullets are inserted just before the next\n",
    "    section header in the README.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : str\n",
    "        Top-level directory name. Currently, only 'processing' and 'analysis'\n",
    "        are supported.\n",
    "    missing_dirs : list of pathlib.Path\n",
    "        List of subdirectory paths that should be added to the README as bullet points.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - For 'processing', bullets are added under the section titled\n",
    "      '## Subfolder descriptions'.\n",
    "    - Each bullet is formatted as:\n",
    "        * [<subdirectory_name>](<repo_url>/<subdirectory_path>/): # TODO!!\n",
    "    - The function preserves all other content and headers in the README.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # load resources to get GH URL\n",
    "    m = load_resources()   \n",
    "    inserted = False\n",
    "    \n",
    "    \n",
    "    # if d == 'processing':\n",
    "    header = \"## Subfolder descriptions\"\n",
    "    # elif d == 'analysis':\n",
    "        # raise ValueError('You havent made this yet')\n",
    "    \n",
    "    # Read the README\n",
    "    readme = f'{d}/README.md'\n",
    "    with open(readme, 'r') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    output_lines = []\n",
    "    inside_section = False\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        output_lines.append(line)\n",
    "\n",
    "        # find first relevant bullet\n",
    "        if header in line:\n",
    "            inside_section = True\n",
    "            continue\n",
    "\n",
    "        # Detect end of bullet list (next header)\n",
    "        if inside_section:\n",
    "            if line.startswith(\"## \"):\n",
    "                # insert new bullets just before the break\n",
    "                last_bullet_idx = max(i for i, line in enumerate(output_lines) if \"* [\"  in line.strip())\n",
    "                for i, sub_d in enumerate(missing_dirs):\n",
    "                    stem_sub_d = sub_d.stem\n",
    "                    output_lines.insert(last_bullet_idx+i+1, f\"* [{stem_sub_d}]({stem_sub_d}/): # TODO!! \")\n",
    "                output_lines.insert(-2, '\\n')\n",
    "                inside_section = False\n",
    "                inserted = True\n",
    "    \n",
    "    # if we had to wait for end of file\n",
    "    if inserted == False:\n",
    "        for i, sub_d in enumerate(missing_dirs):\n",
    "            stem_sub_d = sub_d.stem\n",
    "            output_lines.append(f\"* [{stem_sub_d}]({stem_sub_d}/): # TODO!! \")\n",
    "            output_lines.append('\\n')\n",
    "\n",
    "    # Write back updated README\n",
    "    with open(readme, 'w') as outfile:\n",
    "        outfile.writelines(output_lines)\n",
    "                     \n",
    "    # write to user where README entries have been written\n",
    "    if len(missing_dirs) > 0:\n",
    "        print(f\"Added README entries to {readme} for \")\n",
    "        for sub_d in missing_dirs:\n",
    "            print(f'- {sub_d.stem}')\n",
    "        print()\n",
    "        \n",
    "# all missing dirs to output note to user eventually\n",
    "all_missing_dirs = []\n",
    "\n",
    "# processing\n",
    "d = 'processing'\n",
    "missing_dirs = find_missing_subdirs(d)\n",
    "add_missing_subdirs_to_readme(d, missing_dirs)\n",
    "all_missing_dirs += missing_dirs\n",
    "\n",
    "# analysis\n",
    "d = 'analysis'\n",
    "missing_dirs = find_missing_subdirs(d)\n",
    "add_missing_subdirs_to_readme(d, missing_dirs)\n",
    "all_missing_dirs += missing_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8e964e39-bdf2-45fb-837a-66ce25470ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add links to all relevant unlinked files and directories in READMEs\n",
    "# anchored \"normal\" pattern\n",
    "_pat_normal = re.compile(\n",
    "    r'^([\\W\\s]*?)'              # leading non-word/space\n",
    "    r'((?:\\.\\./)*\\.?[\\w./-]+?)' # the path-ish thing (lazy)\n",
    "    r'([^\\w./-]*)$'             # trailing punctuation\n",
    ")\n",
    "\n",
    "# fallback: find candidate path-like substrings\n",
    "_candidate_re = re.compile(r'(?:\\.\\./)*\\.?[\\w./-]+')\n",
    "\n",
    "_PUNCT_TO_TRIM = set('.,:;)]}\\'\"')\n",
    "\n",
    "def _trim_trailing_punct(core: str):\n",
    "    trailing = []\n",
    "    while core and core[-1] in _PUNCT_TO_TRIM:\n",
    "        trailing.append(core[-1])\n",
    "        core = core[:-1]\n",
    "    return core, ''.join(reversed(trailing))\n",
    "\n",
    "def extract_parts(s: str):\n",
    "    \"\"\"\n",
    "    Extract (leading, core_path, trailing) from a token string.\n",
    "    Returns None if no path-like core is found.\n",
    "    \"\"\"\n",
    "    m = _pat_normal.match(s)\n",
    "    if m:\n",
    "        p1, core, p2 = m.groups()\n",
    "        core, extra = _trim_trailing_punct(core)\n",
    "        p2 = extra + p2\n",
    "        return p1, core, p2\n",
    "\n",
    "    candidates = list(_candidate_re.finditer(s))\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    best = max(candidates, key=lambda mm: mm.end() - mm.start())\n",
    "    core = best.group(0)\n",
    "    start, end = best.start(), best.end()\n",
    "    core_trimmed, extra = _trim_trailing_punct(core)\n",
    "    leading = s[:start]\n",
    "    trailing = extra + s[end:]\n",
    "    return leading, core_trimmed, trailing\n",
    "\n",
    "\n",
    "def format_link(word):\n",
    "    \"\"\"Format link in markdown depending on if it's a directory or a file.\"\"\"\n",
    "    if Path(word).is_dir():\n",
    "        return f'[{word}]({word})'\n",
    "    else:\n",
    "        return f'[`{word}`]({word})'\n",
    "\n",
    "\n",
    "# ---- main transformer ----\n",
    "\n",
    "def link_files(node, files):\n",
    "    \"\"\"\n",
    "    Walk a Markdown AST node and replace unlinked filenames/dirs\n",
    "    with markdown links to those paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node : mdit_py_plugins node\n",
    "        AST node to transform.\n",
    "    files : set of str\n",
    "        Set of known file/directory paths to link.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    node : mdit_py_plugins node\n",
    "        Node with links inserted where appropriate.\n",
    "    \"\"\"\n",
    "\n",
    "    # skip entire code blocks\n",
    "    if isinstance(node, (FencedCode, CodeBlock)):\n",
    "        return node\n",
    "\n",
    "    # skip inline code spans\n",
    "    if isinstance(node, CodeSpan):\n",
    "        return node\n",
    "\n",
    "    # skip existing links\n",
    "    if isinstance(node, Link):\n",
    "        return node\n",
    "\n",
    "    if isinstance(node, RawText):\n",
    "        tokens = re.findall(r'\\S+|\\s+', node.children)\n",
    "        new_nodes = []\n",
    "        for tok in tokens:\n",
    "            if tok.isspace():\n",
    "                new_nodes.append(RawText(tok))\n",
    "                continue\n",
    "            parts = extract_parts(tok)\n",
    "            if parts:\n",
    "                p1, core, p2 = parts\n",
    "                if core in files:\n",
    "                    replaced = format_link(core)\n",
    "                    new_nodes.append(RawText(p1 + replaced + p2))\n",
    "                    continue\n",
    "\n",
    "            # nothing to replace\n",
    "            new_nodes.append(RawText(tok))\n",
    "\n",
    "        return new_nodes\n",
    "\n",
    "    # recurse into children\n",
    "    if hasattr(node, \"children\"):\n",
    "        new_children = []\n",
    "        for child in node.children:\n",
    "            replaced = link_files(child, files)\n",
    "            if isinstance(replaced, list):\n",
    "                new_children.extend(replaced)\n",
    "            else:\n",
    "                new_children.append(replaced)\n",
    "        node.children = new_children\n",
    "\n",
    "    return node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0ccbe917-91d3-4fb8-9746-387054e2df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all git files that are md\n",
    "cmd = 'git ls-files'\n",
    "files = run_cmd(cmd).splitlines()\n",
    "dirs = [str(Path(f).parent) for f in files]\n",
    "md_files = [f for f in list(set(files)|set(dirs)) if f != '.' and f.endswith('.md')]\n",
    "\n",
    "# get all git files\n",
    "cmd = 'git ls-files'\n",
    "files = run_cmd(cmd).splitlines()\n",
    "dirs = [str(Path(f).parent) for f in files]\n",
    "files = [f for f in list(set(files)|set(dirs)) if f != '.']\n",
    "dirs = [f'{f}/' for f in files if Path(f).is_dir()] # add the trailing / version of dirs\n",
    "files += dirs \n",
    "\n",
    "# loop through all md files to edit\n",
    "# md_files = ['README_test.md']\n",
    "# md_files = ['TEST_revised.md']\n",
    "# md_files = ['analysis/README.md']\n",
    "for md_file in md_files:\n",
    "    rel_files = []\n",
    "    for file in files:\n",
    "        \n",
    "        # compute relative path from md_file's directory\n",
    "        rel_path = os.path.relpath(file, Path(md_file).parent)\n",
    "        rel_files.append(rel_path)\n",
    "\n",
    "    md = Markdown()\n",
    "    with open(md_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        doc = md.parse(content)\n",
    "\n",
    "    doc = link_files(doc, rel_files)\n",
    "\n",
    "    md = Markdown(renderer=MarkdownRenderer)\n",
    "    doc = md.render(doc)\n",
    "\n",
    "    # write to new md file\n",
    "    with open({md_file}, 'w') as ofile:\n",
    "        ofile.write(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe15e5-1437-4d42-9b50-bda80a05baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gh action that checks for broken links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
